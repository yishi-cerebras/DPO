{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../generations/btlm_beta0.3_shp-temp1.json', 'r') as f:\n",
    "    generations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../generations/prompt.txt', 'r') as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [32:30<00:00,  7.62s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, generation in tqdm(enumerate(generations), total=len(generations)):\n",
    "    policy_first = True\n",
    "    responseA = generation['policy_response']\n",
    "    responseB = generation['chosen_response']\n",
    "    # change the order every time for more reliable result\n",
    "    if i%2 == 0:\n",
    "        responseA, responseB = responseB, responseA\n",
    "        policy_first = False\n",
    "\n",
    "    content = prompt.format(user_query=generation['prompt'], responseA=responseA, responseB=responseB)\n",
    "    chatgpt_eval = openai.ChatCompletion.create(model=\"gpt-4\", messages=[\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ])\n",
    "    chat_eval = chatgpt_eval.choices[0].message.content\n",
    "\n",
    "    gpt_responses.append({\n",
    "        \"prompt\": content,\n",
    "        \"user_query\": generation['prompt'],\n",
    "        \"policy_first\": policy_first,\n",
    "        \"policy_response\": generation['policy_response'],\n",
    "        \"chosen_response\": generation['chosen_response'],\n",
    "        \"chatgpt_eval\": chat_eval,\n",
    "        \"only_answer\": chat_eval[chat_eval.rfind('More helpful'):]\n",
    "    })\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_responses = []\n",
    "directory_path = 'btlm_beta0.3_shp'\n",
    "\n",
    "for name in os.listdir(directory_path):\n",
    "    with open(os.path.join(directory_path, name), 'r') as f:\n",
    "        gpt_response = json.loads(f.read())\n",
    "        gpt_responses.append(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../generations/gpt_eval-btlm_beta0.1_shp-temp1.json', 'w') as json_file:\n",
    "    json.dump(gpt_responses, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../generations/gpt_eval-temp1.json', 'r') as json_file:\n",
    "    gpt_responses = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate: 0.3984375\n"
     ]
    }
   ],
   "source": [
    "count = len(gpt_responses)\n",
    "hit = 0\n",
    "\n",
    "for resp in gpt_responses:\n",
    "    hit += (resp['policy_first'] and resp['only_answer'][-1] == 'A') or (not resp['policy_first'] and resp['only_answer'][-1] == 'B')\n",
    "print(f\"Win rate: {hit/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generations_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gen in generations:\n",
    "#     assistant_word = '\\n\\nAssistant:'\n",
    "#     resp_indx = gen['prompt'][0].rfind(assistant_word)\n",
    "#     prompt = gen['prompt'][0][:resp_indx]\n",
    "#     chosen_response = gen['chosen_response'][0][resp_indx+len(assistant_word):].strip()\n",
    "#     policy_response = gen['policy_response'][0][resp_indx+len(assistant_word):].strip()\n",
    "#     reference_response = gen['reference_response'][0][resp_indx+len(assistant_word):].strip()\n",
    "#     generations_.append({'prompt': prompt, 'chosen_response': chosen_response, 'policy_response': policy_response, 'reference_response': reference_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generations = generations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythia without LoRa float32: 0.46 (Probably something wrong)\n",
    "# Pythia without LoRa bfloat16: 0.73\n",
    "# Original authors model: 0.707\n",
    "# BTLM beta 0.1 LoRa HH: 0.74  rewards/accuracies: 0.6296\n",
    "# BTLM beta 0.3 LoRa HH: 0.59  rewards/accuracies: 0.6166\n",
    "# BTLM beta 0.5 LoRa HH: 0.52  rewards/accuracies: 0.6092\n",
    "# BTLM beta 0.1 LoRa SHP: 0.36\n",
    "# BTLM beta 0.3 LoRa SHP: 0.40\n",
    "# BTLM beta 0.5 LoRa SHP: \n",
    "# SFT BTLM: 0.43\n",
    "# with LoRa and bfloat16: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'btlm_beta0.1_shp'\n",
    "\n",
    "for file in glob(f'{directory_path}/*'):\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except OSError as e:\n",
    "        print(f'Error: {file}: {e.strerror}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('btlm_beta0.1_shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
